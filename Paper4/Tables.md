# Tables

## Required Tables

### Table 1: Sensitivity Analysis Methods Comparison
- **Location**: Analysis Capabilities section
- **Content**: Comparison of 12 sensitivity analysis methods
- **Columns**: Method, Type, Sample Size, Interactions, Accuracy, Best For
- **Data**: Comprehensive comparison of Sobol, Morris, FAST, RBD, Delta, PAWN, DGSM, Local methods, Monte Carlo, Bayesian

### Table 2: Anomaly Detection Methods Comparison
- **Location**: Analysis Capabilities section
- **Content**: Comparison of 18 anomaly detection methods
- **Columns**: Method, Category, Multivariate, Temporal, Complexity, Best For
- **Data**: Comparison of statistical, clustering, ML-based, and rule-based methods

### Table 3: Analysis Method Performance
- **Location**: Results section
- **Content**: Execution times and throughput for different analysis methods
- **Columns**: Analysis Type, Method, Data Size, Execution Time, Throughput
- **Data**: Performance metrics for sensitivity analysis, virtual experiments, and anomaly detection

### Table 4: Warehouse Query Performance
- **Location**: Results section
- **Content**: Query performance for different data sources
- **Columns**: Data Source, Query Type, Points Retrieved, Query Time, Throughput
- **Data**: Performance metrics for laser parameters, ISPM, CT scans, and multi-source queries

### Table 5: Sensitivity Analysis Method Comparison
- **Location**: Results section
- **Content**: Detailed comparison of sensitivity methods on same dataset
- **Columns**: Method, Sample Size, Accuracy, Interactions, Time (s), Best For
- **Data**: Performance and accuracy comparison

### Table 6: Virtual Experiment Design Efficiency
- **Location**: Results section
- **Content**: Comparison of experiment design types
- **Columns**: Design Type, Samples, Coverage, Efficiency, Time (s)
- **Data**: Efficiency metrics for LHS, factorial, response surface, and optimal designs

### Table 7: Anomaly Detection Method Performance
- **Location**: Results section
- **Content**: Performance metrics for anomaly detection methods
- **Columns**: Method, Precision, Recall, F1-Score, False Positive Rate, Time (s)
- **Data**: Validation metrics against CT scan ground truth

### Table 8: Multi-Signal Fusion Strategy Performance
- **Location**: Results section
- **Content**: Comparison of fusion strategies
- **Columns**: Strategy, Coverage, SNR Improvement, Quality Score, Best For
- **Data**: Performance metrics for different fusion strategies

### Table 9: Sensitivity Indices for Temperature Output
- **Location**: Case Studies section
- **Content**: Sobol indices for process variables influencing temperature
- **Columns**: Variable, S1 (First-Order), ST (Total-Order), Interpretation
- **Data**: Sensitivity indices with confidence intervals

### Table 10: Sensitivity Indices for Density Output
- **Location**: Case Studies section
- **Content**: Sobol indices for process variables influencing density
- **Columns**: Variable, S1, ST, Interpretation
- **Data**: Sensitivity indices for density analysis

### Table 11: Sensitivity Indices for Defect Count Output
- **Location**: Case Studies section
- **Content**: Sobol indices for process variables influencing defect formation
- **Columns**: Variable, S1, ST, Interpretation
- **Data**: Sensitivity indices for defect analysis

### Table 12: Anomaly Detection Summary
- **Location**: Case Studies section
- **Content**: Summary of anomaly detection results
- **Columns**: Detection Method, Anomalies Detected, Spatial Clusters, Average Score
- **Data**: Anomaly detection statistics

## Table Captions

**Table 1**: Comparison of 12 sensitivity analysis methods including global, local, and uncertainty-based methods with their characteristics and best use cases.

**Table 2**: Comparison of 18 anomaly detection methods across statistical, clustering, machine learning, and rule-based categories.

**Table 3**: Performance metrics showing execution times and throughput for different analysis methods.

**Table 4**: Warehouse query performance metrics for different data sources and query types.

**Table 5**: Detailed comparison of sensitivity analysis methods on the same dataset, showing accuracy, interaction detection, and execution time.

**Table 6**: Efficiency comparison of virtual experiment design types, showing coverage and execution time.

**Table 7**: Performance metrics for anomaly detection methods validated against CT scan ground truth.

**Table 8**: Performance comparison of multi-signal fusion strategies, showing coverage improvement and quality scores.

**Table 9**: Sobol sensitivity indices (first-order S1 and total-order ST) for process variables influencing temperature output.

**Table 10**: Sobol sensitivity indices for process variables influencing density output.

**Table 11**: Sobol sensitivity indices for process variables influencing defect count output.

**Table 12**: Summary of anomaly detection results showing detection counts, spatial clusters, and average scores for different methods.

